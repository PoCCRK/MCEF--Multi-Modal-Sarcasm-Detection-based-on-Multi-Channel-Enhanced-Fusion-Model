{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521697eb-db06-4cad-9759-0d52467b4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "seed = 777\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "pretrained_bert_name = '/hy-tmp/models/bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "max_seq_len = 100\n",
    "\n",
    "pretrained_vit_name = '/hy-tmp/models/vit-base-patch16-224'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(pretrained_vit_name)\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "crop_size = (feature_extractor.size['height'], feature_extractor.size['width'])\n",
    "# crop_size = feature_extractor.size\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(crop_size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "img_dir = '/hy-tmp/data/dataset_image'\n",
    "train_file = '/hy-tmp/data/processed_train.data'\n",
    "valid_file = '/hy-tmp/data/processed_valid.data'\n",
    "test_file = '/hy-tmp/data/processed_test.data'\n",
    "\n",
    "model_name = 'CM_ATTENTION4'\n",
    "check_point_path = '/hy-tmp/models'\n",
    "log_file = f'/root/logs/{model_name}-{strftime(\"%y%m%d-%H%M\", localtime())}.log'\n",
    "result_file = f'/root/results/{model_name}_predicts.txt'\n",
    "model_checkpoint = f'{check_point_path}/best_state/{model_name}'\n",
    "\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "inputs_cols = ['labels', 'box_vit', 'images', 'text_indices', 'text_in_img_indices', 'text_merge_indices', 'attribute_object_indices']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_dim = 768\n",
    "vit_dim = 768\n",
    "polarities_dim = 2\n",
    "hidden_dim = 512\n",
    "batch_size = 16\n",
    "dropout = 0.1\n",
    "patch_size = 16\n",
    "val_step = 40\n",
    "\n",
    "sp_nlp = spacy.load('en_core_web_sm')\n",
    "filenames = os.listdir(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9f3f82-c3ef-41db-8270-232a99f88077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(text, max_len=0):\n",
    "    token_list = []\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    document = sp_nlp(text)\n",
    "    spacy_token = [str(x) for x in document]\n",
    "    spacy_len = len(spacy_token)\n",
    "    \n",
    "    # if max_len > 0:\n",
    "    #     if spacy_len > max_len:\n",
    "    #         spacy_token = spacy_token[:max_len]\n",
    "\n",
    "    s = ''\n",
    "    for token in spacy_token:\n",
    "        s = s + ' ' + token\n",
    "    # document = sp_nlp(s)\n",
    "    # spacy_token = [str(x) for x in document]\n",
    "    return document, s.strip(), spacy_token\n",
    "\n",
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x\n",
    "\n",
    "class attention_Dataset(Dataset):\n",
    "    def __init__(self, data_file, img_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        data = pickle.load(open(data_file,'rb'))\n",
    "\n",
    "        print(\"{}.data\".format(data_file))\n",
    "        all_data = []\n",
    "        for key,value in data.items():\n",
    "            img_id = value['id']\n",
    "            label = int(value['label'])\n",
    "            image_file = img_id+'.jpg'\n",
    "            \n",
    "            text = value['text']\n",
    "            attribute_objects = value['attribute_objects']\n",
    "            text_in_img = value['text_in_img']\n",
    "            box_vit = value[\"box_vit\"]\n",
    "            box_vit = [x.numpy() for x in box_vit]\n",
    "            \n",
    "            data_ = {\n",
    "                'img_id': img_id,\n",
    "                'label':label,\n",
    "                'box_vit':box_vit,\n",
    "                'image_file': image_file,\n",
    "                'text':text,\n",
    "                'text_in_img':text_in_img,\n",
    "                'attribute_objects':attribute_objects,\n",
    "            }\n",
    "            all_data.append(data_)\n",
    "        self.all_data = all_data\n",
    "     \n",
    "    def text_to_indices(self, text, text_pair=None):\n",
    "        if text_pair is None:\n",
    "            encoded_dict = self.tokenizer(\n",
    "                                text,                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                padding = 'max_length',\n",
    "                                truncation = True,\n",
    "                                max_length = max_seq_len,    # Pad & truncate all sentences.\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                return_tensors = 'np',     # Return pytorch tensors.\n",
    "                                return_length = True,\n",
    "                                is_split_into_words = True,\n",
    "                           )\n",
    "\n",
    "        else:\n",
    "            encoded_dict = self.tokenizer(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            text_pair,\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            padding = 'max_length',\n",
    "                            truncation = 'longest_first',\n",
    "                            max_length = max_seq_len,    # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'np',     # Return pytorch tensors.\n",
    "                            return_length = True,\n",
    "                            is_split_into_words = True,\n",
    "                       )\n",
    "        return encoded_dict\n",
    "    \n",
    "    def my_collate_fn(self, data):\n",
    "        # use bert tokenizer, no graph returned\n",
    "        b_img_id = []\n",
    "        b_label = []\n",
    "        b_box_vit = []\n",
    "        b_image = []\n",
    "        b_text_indices = []\n",
    "        b_text_in_img_indices = []\n",
    "        b_text_merge_indices = []\n",
    "        b_attribute_object_indices = []\n",
    "\n",
    "        for item in data:\n",
    "            b_img_id.append(item['img_id'])\n",
    "            b_label.append(item['label'])\n",
    "            b_box_vit.append(item['box_vit'])\n",
    "            b_image.append(item['image'])\n",
    "\n",
    "            text = item['text']\n",
    "            text_in_img = item['text_in_img']\n",
    "            attribute_objects = item['attribute_objects']\n",
    "            \n",
    "            attribute_objects_token = []\n",
    "            for attribute, _object in attribute_objects:\n",
    "                attribute_objects_token += [attribute, _object]\n",
    "\n",
    "            text_doc,_,text_token = get_doc(text)\n",
    "            text_in_img_doc,_,text_in_img_token = get_doc(text_in_img)\n",
    "            if not text_token:\n",
    "                text_token = ['']\n",
    "            if not text_in_img_token:\n",
    "                text_in_img_token = ['']\n",
    "            \n",
    "            b_text_indices.append(text_token)\n",
    "            b_text_in_img_indices.append(text_in_img_token)\n",
    "            b_attribute_object_indices.append(attribute_objects_token)\n",
    "        \n",
    "        text_encoded_dict = self.text_to_indices(b_text_indices)\n",
    "        text_in_img_encoded_dict = self.text_to_indices(b_text_in_img_indices)\n",
    "        text_merge_encoded_dict = self.text_to_indices(b_text_indices, b_text_in_img_indices)\n",
    "        attribute_object_encoded_dict = self.text_to_indices(b_attribute_object_indices)\n",
    "        \n",
    "        return {\n",
    "                    'img_ids': b_img_id,\n",
    "                    'labels': torch.tensor(b_label),\n",
    "                    'box_vit':torch.tensor(np.array(b_box_vit)),\n",
    "                    'images':torch.stack(b_image, dim=0),\n",
    "                    'text_indices':torch.tensor(text_encoded_dict.input_ids),\n",
    "                    'text_in_img_indices':torch.tensor(text_in_img_encoded_dict.input_ids),\n",
    "                    'text_merge_indices':torch.tensor(text_merge_encoded_dict.input_ids),\n",
    "                    'attribute_object_indices':torch.tensor(attribute_object_encoded_dict.input_ids),\n",
    "                    }\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.all_data[index]['img_id']\n",
    "        image_file = self.all_data[index]['image_file']\n",
    "        label = self.all_data[index]['label']\n",
    "        box_vit = self.all_data[index]['box_vit']\n",
    "        image = Image.open(os.path.join(self.img_dir, image_file))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.all_data[index]['text']\n",
    "        text_in_img = self.all_data[index]['text_in_img']\n",
    "        attribute_objects = self.all_data[index]['attribute_objects']\n",
    "        \n",
    "        return {\n",
    "            'img_id': img_id,\n",
    "            'label':label,\n",
    "            'box_vit':box_vit,\n",
    "            'image': image,\n",
    "            'text':text,\n",
    "            'text_in_img':text_in_img,\n",
    "            'attribute_objects':attribute_objects,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3b797f-f4c5-49dd-80fd-6aea10c4096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hy-tmp/data/processed_train.data.data\n",
      "/hy-tmp/data/processed_valid.data.data\n",
      "/hy-tmp/data/processed_test.data.data\n",
      "19816 2410 2409\n"
     ]
    }
   ],
   "source": [
    "train_dataset = attention_Dataset(data_file=train_file, img_dir=img_dir, transform=train_transforms)\n",
    "valid_dataset = attention_Dataset(data_file=valid_file, img_dir=img_dir, transform=val_transforms)\n",
    "test_dataset = attention_Dataset(data_file=test_file, img_dir=img_dir, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.my_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=valid_dataset.my_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.my_collate_fn)\n",
    "\n",
    "print(train_dataset.__len__(), valid_dataset.__len__(), test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67873070-ca90-46ae-8e73-1e5c9b48a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedScaledDotProductAttention(nn.Module):\n",
    "    '''\n",
    "    Scaled dot-product attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, h, dropout=.1):\n",
    "        '''\n",
    "        :param d_model: Output dimensionality of the model\n",
    "        :param d_k: Dimensionality of queries and keys\n",
    "        :param d_v: Dimensionality of values\n",
    "        :param h: Number of heads\n",
    "        '''\n",
    "        super(SimplifiedScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model//h\n",
    "        self.d_v = d_model//h\n",
    "        self.h = h\n",
    "\n",
    "        self.fc_o = nn.Linear(h * self.d_v, d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "        '''\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n",
    "        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n",
    "        :return:\n",
    "        '''\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = queries.view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = keys.view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = values.view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "        if attention_mask is not None:\n",
    "            att = att.masked_fill(attention_mask, -np.inf)\n",
    "        \n",
    "        # we activate the negative att\n",
    "        att = torch.abs(att)\n",
    "        \n",
    "        att = torch.softmax(att, -1)\n",
    "        \n",
    "        att=self.dropout(att)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "class DynamicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n",
    "                 bidirectional=False, only_use_last_hidden_state=False, rnn_type = 'LSTM'):\n",
    "        \"\"\"\n",
    "        LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, length...).\n",
    "\n",
    "        :param input_size:The number of expected features in the input x\n",
    "        :param hidden_size:The number of features in the hidden state h\n",
    "        :param num_layers:Number of recurrent layers.\n",
    "        :param bias:If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "        :param batch_first:If True, then the input and output tensors are provided as (batch, seq, feature)\n",
    "        :param dropout:If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
    "        :param bidirectional:If True, becomes a bidirectional RNN. Default: False\n",
    "        :param rnn_type: {LSTM, GRU, RNN}\n",
    "        \"\"\"\n",
    "        super(DynamicLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.only_use_last_hidden_state = only_use_last_hidden_state\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        if self.rnn_type == 'LSTM': \n",
    "            self.RNN = nn.LSTM(\n",
    "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)  \n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.RNN = nn.GRU(\n",
    "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'RNN':\n",
    "            self.RNN = nn.RNN(\n",
    "                input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        \n",
    "\n",
    "    def forward(self, x, x_len, h0=None):\n",
    "        \"\"\"\n",
    "        sequence -> sort -> pad and pack ->process using RNN -> unpack ->unsort\n",
    "\n",
    "        :param x: sequence embedding vectors\n",
    "        :param x_len: numpy/tensor list\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"sort\"\"\"\n",
    "        x_sort_idx = torch.argsort(-x_len)\n",
    "        x_unsort_idx = torch.argsort(x_sort_idx).long()\n",
    "        x_len = x_len[x_sort_idx]\n",
    "        x = x[x_sort_idx.long()]\n",
    "        \"\"\"pack\"\"\"\n",
    "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
    "        \n",
    "        if self.rnn_type == 'LSTM':\n",
    "            if h0 is None: \n",
    "                out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n",
    "            else:\n",
    "                out_pack, (ht, ct) = self.RNN(x_emb_p, (h0, h0))\n",
    "        else: \n",
    "            if h0 is None:\n",
    "                out_pack, ht = self.RNN(x_emb_p, None)\n",
    "            else:\n",
    "                out_pack, ht = self.RNN(x_emb_p, h0)\n",
    "            ct = None\n",
    "        \"\"\"unsort: h\"\"\"\n",
    "        ht = torch.transpose(ht, 0, 1)[\n",
    "            x_unsort_idx]  \n",
    "        ht = torch.transpose(ht, 0, 1)\n",
    "\n",
    "        if self.only_use_last_hidden_state:\n",
    "            return ht\n",
    "        else:\n",
    "            \"\"\"unpack: out\"\"\"\n",
    "            out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)\n",
    "            out = out[0]  #\n",
    "            out = out[x_unsort_idx]\n",
    "            \"\"\"unsort: out c\"\"\"\n",
    "            if self.rnn_type =='LSTM':\n",
    "                ct = torch.transpose(ct, 0, 1)[\n",
    "                    x_unsort_idx]\n",
    "                ct = torch.transpose(ct, 0, 1)\n",
    "\n",
    "            return out, (ht, ct)\n",
    "        \n",
    "class CM_ATTENTION4(nn.Module):\n",
    "    def __init__(self, pretrained_bert_name, pretrained_vit_name):\n",
    "        super(CM_ATTENTION4, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_bert_name)\n",
    "        self.vit = ViTModel.from_pretrained(pretrained_vit_name)\n",
    "        self.attention1 = SimplifiedScaledDotProductAttention(d_model=bert_dim, h=2)\n",
    "        self.attention2 = SimplifiedScaledDotProductAttention(d_model=bert_dim, h=2)\n",
    "        \n",
    "        self.lstm1 = DynamicLSTM(bert_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = DynamicLSTM(vit_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, polarities_dim)\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.params = []\n",
    "        for child in self.children():\n",
    "            if child != self.bert and child != self.vit:\n",
    "                self.params += child.parameters()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        labels, box_vit, images, text_indices, text_in_img_indices, text_merge_indices, attribute_object_indices = inputs\n",
    "        bs = labels.shape[0]\n",
    "        box_vit_len = torch.tensor([10]*bs)\n",
    "        img_patch_len = torch.tensor([patch_size]*bs)\n",
    "        text_len = torch.sum(text_indices != 0, dim=-1)\n",
    "        text_in_img_len = torch.sum(text_in_img_indices != 0, dim=-1)\n",
    "        text_merge_len = torch.sum(text_merge_indices != 0, dim=-1)\n",
    "        attribute_object_len = torch.sum(attribute_object_indices != 0, dim=-1)\n",
    "        \n",
    "        text_out = self.bert(text_merge_indices,  output_hidden_states=False)\n",
    "        image_out = self.vit(images, output_hidden_states=False)\n",
    "        \n",
    "        atte_text = self.attention1(queries=image_out.last_hidden_state, keys=text_out.last_hidden_state,\n",
    "                                     values=text_out.last_hidden_state) + image_out.last_hidden_state\n",
    "        atte_image = self.attention2(queries=text_out.last_hidden_state, keys=image_out.last_hidden_state,\n",
    "                                     values=image_out.last_hidden_state) + text_out.last_hidden_state\n",
    "        \n",
    "        atte_text_out, (_, _) = self.lstm1(atte_text, img_patch_len.cpu())\n",
    "        atte_image_out, (_, _) = self.lstm2(atte_image, text_merge_len.cpu())\n",
    "        \n",
    "        feature = torch.cat([atte_text_out[:,0,:], atte_image_out[:,0,:]], dim = 1)\n",
    "        feature = self.dropout(feature)\n",
    "        \n",
    "        output = self.fc2(F.relu(self.fc1(feature)))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def reset_params(self):\n",
    "        layers = [self.lstm1, self.lstm2, self.fc1, self.fc2]\n",
    "        for layer in layers:\n",
    "            modules = layer.modules()\n",
    "            for m in modules:\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    init.constant_(m.weight, 1)\n",
    "                    init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    init.normal_(m.weight, std=0.001)\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be44efc-637d-4fc3-b432-80a32b568f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_(model, data_loader, save_path=None):\n",
    "    n_correct, n_total = 0, 0\n",
    "    t_targets_all, t_outputs_all = None, None\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_batch, t_batch in enumerate(data_loader):\n",
    "            t_inputs = [t_batch[col].to(device)   for col in inputs_cols]\n",
    "            t_targets = t_batch['labels'].to(device)\n",
    "            t_img_ids = t_batch['img_ids']\n",
    "            \n",
    "            t_outputs = model(t_inputs)\n",
    "\n",
    "            n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "            n_total += len(t_outputs)\n",
    "\n",
    "            if t_targets_all is None:\n",
    "                t_targets_all = t_targets\n",
    "                t_outputs_all = t_outputs\n",
    "                t_img_ids_all = t_img_ids\n",
    "            else:\n",
    "                t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "                t_img_ids_all += t_img_ids\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path,'w',encoding='utf-8') as fout:\n",
    "            img_ids_all = t_img_ids_all\n",
    "            predicts_all = torch.argmax(t_outputs_all, -1).cpu().numpy().tolist()\n",
    "            labels_all = t_targets_all.cpu().numpy().tolist()\n",
    "            outputs_all = t_outputs_all.cpu().numpy().tolist()\n",
    "            assert len(img_ids_all) == len(predicts_all) == len(labels_all) == len(outputs_all)\n",
    "            \n",
    "            for i in range(len(img_ids_all)):\n",
    "                img_id = img_ids_all[i]\n",
    "                predict = predicts_all[i]\n",
    "                label = labels_all[i]\n",
    "                output = outputs_all[i]\n",
    "                fout.write(f'{str(img_id)} {str(predict)} {str(label)} {str(output)} \\n')\n",
    "\n",
    "    acc = n_correct / n_total\n",
    "    f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu())\n",
    "    precision =  metrics.precision_score(t_targets_all.cpu(),torch.argmax(t_outputs_all, -1).cpu())\n",
    "    recall = metrics.recall_score(t_targets_all.cpu(),torch.argmax(t_outputs_all, -1).cpu())\n",
    "    return acc, f1 ,precision, recall\n",
    "\n",
    "def train(model, train_data_loader, val_data_loader, test_data_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam([{'params':model.bert.parameters(),'lr':2e-5},\n",
    "                                  {'params':model.vit.parameters(),'lr':2e-5},\n",
    "                                  {'params':model.params,'lr':1e-3},\n",
    "                                 ], lr=1e-3, weight_decay=1e-5)\n",
    "    global_step = 0\n",
    "    max_val_acc = 0\n",
    "    max_val_f1 = 0\n",
    "    max_val_epoch = 0\n",
    "    \n",
    "    model.reset_params()\n",
    "    \n",
    "    for i_epoch in range(100):\n",
    "        logger.info('>' * 100)\n",
    "        logger.info('epoch: {}'.format(i_epoch))\n",
    "        n_correct, n_total, loss_total = 0, 0, 0\n",
    "\n",
    "        for i_batch, batch in enumerate(train_data_loader):\n",
    "            model.train()\n",
    "            global_step += 1\n",
    "\n",
    "            inputs = [batch[col].to(device)   for col in inputs_cols]\n",
    "            outputs = model(inputs)\n",
    "            targets = batch['labels'].to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "            n_total += len(outputs)\n",
    "            loss_total += loss.item() * len(outputs)\n",
    "\n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "            logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "            if global_step % val_step == 0:\n",
    "                val_acc, val_f1,val_precision,val_recall = eval_(model, val_data_loader)\n",
    "                logger.info('> max_val_f1: {:.4f}, max_val_acc: {:.4f}'.format(max_val_f1,max_val_acc))\n",
    "                logger.info('> val_acc: {:.4f}, val_f1: {:.4f}, val_precision: {:.4f}, val_recall: {:.4f}'.format(val_acc,val_f1,val_precision,val_recall))\n",
    "\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_f1 = val_f1\n",
    "                    max_val_acc = val_acc\n",
    "                    max_val_epoch = i_epoch\n",
    "                    \n",
    "                    torch.save(model.state_dict(), model_checkpoint)\n",
    "                    logger.info(f'>> saved: {model_checkpoint}')\n",
    "\n",
    "        if i_epoch - max_val_epoch >= 3:\n",
    "            logger.info('>> early stop.')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(model_checkpoint))\n",
    "    model = model.to(device)\n",
    "\n",
    "    test_acc, test_f1,test_precision,test_recall = eval_(model, test_data_loader, save_path=result_file)\n",
    "    \n",
    "    logger.info(f\"{test_acc} {test_f1} {test_precision} {test_recall}\")\n",
    "\n",
    "    return (test_acc, test_f1,test_precision,test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef62191-1933-4211-88ef-cd4b27975bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "loss: 0.6931, acc: 0.6250\n",
      "loss: 0.6942, acc: 0.5312\n",
      "loss: 0.6936, acc: 0.6042\n",
      "loss: 0.6912, acc: 0.6406\n",
      "loss: 0.6926, acc: 0.6000\n",
      "loss: 0.6930, acc: 0.5729\n",
      "loss: 0.6891, acc: 0.6161\n",
      "loss: 0.6856, acc: 0.6172\n",
      "loss: 0.6766, acc: 0.6181\n",
      "loss: 0.6743, acc: 0.6125\n",
      "loss: 0.6605, acc: 0.6193\n",
      "loss: 0.6513, acc: 0.6250\n",
      "loss: 0.6363, acc: 0.6442\n",
      "loss: 0.6246, acc: 0.6518\n",
      "loss: 0.6104, acc: 0.6625\n",
      "loss: 0.6183, acc: 0.6641\n",
      "loss: 0.6306, acc: 0.6618\n",
      "loss: 0.6383, acc: 0.6632\n",
      "loss: 0.6269, acc: 0.6711\n",
      "loss: 0.6231, acc: 0.6719\n",
      "loss: 0.6184, acc: 0.6786\n",
      "loss: 0.6065, acc: 0.6875\n",
      "loss: 0.6050, acc: 0.6929\n",
      "loss: 0.6192, acc: 0.6875\n",
      "loss: 0.6165, acc: 0.6850\n",
      "loss: 0.6166, acc: 0.6827\n",
      "loss: 0.6170, acc: 0.6829\n",
      "loss: 0.6156, acc: 0.6853\n",
      "loss: 0.6146, acc: 0.6875\n",
      "loss: 0.6164, acc: 0.6833\n",
      "loss: 0.6163, acc: 0.6835\n",
      "loss: 0.6161, acc: 0.6836\n",
      "loss: 0.6155, acc: 0.6837\n",
      "loss: 0.6197, acc: 0.6765\n",
      "loss: 0.6193, acc: 0.6768\n",
      "loss: 0.6161, acc: 0.6806\n",
      "loss: 0.6130, acc: 0.6841\n",
      "loss: 0.6121, acc: 0.6842\n",
      "loss: 0.6111, acc: 0.6859\n",
      "loss: 0.6108, acc: 0.6844\n",
      "> max_val_f1: 0.0000, max_val_acc: 0.0000\n",
      "> val_acc: 0.6842, val_f1: 0.5751, val_precision: 0.6190, val_recall: 0.5370\n",
      ">> saved: /hy-tmp/models/best_state/CM_ATTENTION4\n",
      "loss: 0.6135, acc: 0.6845\n",
      "loss: 0.6134, acc: 0.6830\n",
      "loss: 0.6087, acc: 0.6875\n",
      "loss: 0.6061, acc: 0.6903\n",
      "loss: 0.6037, acc: 0.6931\n",
      "loss: 0.6016, acc: 0.6929\n",
      "loss: 0.5996, acc: 0.6928\n",
      "loss: 0.5970, acc: 0.6927\n",
      "loss: 0.5978, acc: 0.6964\n",
      "loss: 0.5993, acc: 0.6987\n",
      "loss: 0.5972, acc: 0.6985\n",
      "loss: 0.5990, acc: 0.6995\n",
      "loss: 0.6009, acc: 0.6958\n",
      "loss: 0.5989, acc: 0.6979\n",
      "loss: 0.5990, acc: 0.6966\n",
      "loss: 0.5969, acc: 0.6975\n",
      "loss: 0.5988, acc: 0.6985\n",
      "loss: 0.5960, acc: 0.6994\n",
      "loss: 0.5968, acc: 0.6960\n",
      "loss: 0.5947, acc: 0.6969\n",
      "loss: 0.5926, acc: 0.6977\n",
      "loss: 0.5905, acc: 0.6996\n",
      "loss: 0.5881, acc: 0.7024\n",
      "loss: 0.5850, acc: 0.7041\n",
      "loss: 0.5855, acc: 0.7029\n",
      "loss: 0.5845, acc: 0.7036\n",
      "loss: 0.5865, acc: 0.7024\n",
      "loss: 0.5839, acc: 0.7040\n",
      "loss: 0.5821, acc: 0.7056\n",
      "loss: 0.5808, acc: 0.7063\n",
      "loss: 0.5797, acc: 0.7069\n",
      "loss: 0.5762, acc: 0.7109\n",
      "loss: 0.5771, acc: 0.7106\n",
      "loss: 0.5793, acc: 0.7095\n",
      "loss: 0.5774, acc: 0.7108\n",
      "loss: 0.5802, acc: 0.7089\n",
      "loss: 0.5814, acc: 0.7086\n",
      "loss: 0.5808, acc: 0.7083\n",
      "loss: 0.5796, acc: 0.7089\n",
      "loss: 0.5783, acc: 0.7109\n",
      "> max_val_f1: 0.5751, max_val_acc: 0.6842\n",
      "> val_acc: 0.7224, val_f1: 0.5458, val_precision: 0.7821, val_recall: 0.4192\n",
      ">> saved: /hy-tmp/models/best_state/CM_ATTENTION4\n",
      "loss: 0.5783, acc: 0.7099\n",
      "loss: 0.5803, acc: 0.7088\n",
      "loss: 0.5816, acc: 0.7086\n",
      "loss: 0.5812, acc: 0.7076\n",
      "loss: 0.5798, acc: 0.7081\n",
      "loss: 0.5801, acc: 0.7078\n",
      "loss: 0.5791, acc: 0.7098\n",
      "loss: 0.5784, acc: 0.7102\n",
      "loss: 0.5777, acc: 0.7107\n",
      "loss: 0.5773, acc: 0.7104\n",
      "loss: 0.5760, acc: 0.7122\n",
      "loss: 0.5763, acc: 0.7113\n",
      "loss: 0.5735, acc: 0.7124\n",
      "loss: 0.5746, acc: 0.7108\n",
      "loss: 0.5787, acc: 0.7079\n",
      "loss: 0.5764, acc: 0.7103\n",
      "loss: 0.5782, acc: 0.7094\n",
      "loss: 0.5779, acc: 0.7098\n",
      "loss: 0.5787, acc: 0.7090\n",
      "loss: 0.5774, acc: 0.7106\n",
      "loss: 0.5801, acc: 0.7092\n",
      "loss: 0.5792, acc: 0.7102\n",
      "loss: 0.5804, acc: 0.7093\n",
      "loss: 0.5797, acc: 0.7109\n",
      "loss: 0.5792, acc: 0.7119\n",
      "loss: 0.5802, acc: 0.7093\n",
      "loss: 0.5801, acc: 0.7097\n",
      "loss: 0.5794, acc: 0.7118\n",
      "loss: 0.5786, acc: 0.7127\n",
      "loss: 0.5786, acc: 0.7125\n",
      "loss: 0.5787, acc: 0.7128\n",
      "loss: 0.5802, acc: 0.7115\n",
      "loss: 0.5792, acc: 0.7118\n",
      "loss: 0.5792, acc: 0.7111\n",
      "loss: 0.5781, acc: 0.7120\n",
      "loss: 0.5765, acc: 0.7134\n",
      "loss: 0.5780, acc: 0.7126\n",
      "loss: 0.5765, acc: 0.7135\n",
      "loss: 0.5748, acc: 0.7148\n",
      "loss: 0.5758, acc: 0.7135\n",
      "> max_val_f1: 0.5458, max_val_acc: 0.7224\n",
      "> val_acc: 0.7216, val_f1: 0.7027, val_precision: 0.6109, val_recall: 0.8269\n",
      "loss: 0.5760, acc: 0.7123\n",
      "loss: 0.5749, acc: 0.7121\n",
      "loss: 0.5739, acc: 0.7124\n",
      "loss: 0.5718, acc: 0.7137\n",
      "loss: 0.5704, acc: 0.7145\n",
      "loss: 0.5707, acc: 0.7148\n",
      "loss: 0.5698, acc: 0.7170\n",
      "loss: 0.5681, acc: 0.7178\n",
      "loss: 0.5673, acc: 0.7175\n",
      "loss: 0.5657, acc: 0.7183\n",
      "loss: 0.5658, acc: 0.7176\n",
      "loss: 0.5632, acc: 0.7192\n",
      "loss: 0.5648, acc: 0.7190\n",
      "loss: 0.5637, acc: 0.7201\n",
      "loss: 0.5645, acc: 0.7213\n",
      "loss: 0.5629, acc: 0.7215\n",
      "loss: 0.5603, acc: 0.7231\n",
      "loss: 0.5595, acc: 0.7237\n",
      "loss: 0.5596, acc: 0.7230\n",
      "loss: 0.5586, acc: 0.7237\n",
      "loss: 0.5594, acc: 0.7230\n",
      "loss: 0.5581, acc: 0.7245\n",
      "loss: 0.5581, acc: 0.7242\n",
      "loss: 0.5574, acc: 0.7248\n",
      "loss: 0.5569, acc: 0.7250\n",
      "loss: 0.5557, acc: 0.7256\n",
      "loss: 0.5554, acc: 0.7258\n",
      "loss: 0.5564, acc: 0.7251\n",
      "loss: 0.5560, acc: 0.7253\n",
      "loss: 0.5547, acc: 0.7262\n",
      "loss: 0.5555, acc: 0.7256\n",
      "loss: 0.5539, acc: 0.7270\n",
      "loss: 0.5552, acc: 0.7267\n",
      "loss: 0.5538, acc: 0.7277\n",
      "loss: 0.5532, acc: 0.7278\n",
      "loss: 0.5518, acc: 0.7288\n",
      "loss: 0.5522, acc: 0.7281\n",
      "loss: 0.5512, acc: 0.7290\n",
      "loss: 0.5512, acc: 0.7284\n",
      "loss: 0.5496, acc: 0.7293\n",
      "> max_val_f1: 0.5458, max_val_acc: 0.7224\n",
      "> val_acc: 0.7602, val_f1: 0.7133, val_precision: 0.6802, val_recall: 0.7497\n",
      ">> saved: /hy-tmp/models/best_state/CM_ATTENTION4\n",
      "loss: 0.5481, acc: 0.7306\n",
      "loss: 0.5490, acc: 0.7303\n",
      "loss: 0.5470, acc: 0.7316\n",
      "loss: 0.5464, acc: 0.7321\n",
      "loss: 0.5456, acc: 0.7330\n",
      "loss: 0.5447, acc: 0.7338\n",
      "loss: 0.5421, acc: 0.7354\n",
      "loss: 0.5423, acc: 0.7355\n",
      "loss: 0.5415, acc: 0.7359\n",
      "loss: 0.5392, acc: 0.7371\n",
      "loss: 0.5393, acc: 0.7376\n",
      "loss: 0.5410, acc: 0.7369\n",
      "loss: 0.5427, acc: 0.7370\n",
      "loss: 0.5415, acc: 0.7378\n",
      "loss: 0.5397, acc: 0.7386\n",
      "loss: 0.5379, acc: 0.7397\n",
      "loss: 0.5374, acc: 0.7401\n",
      "loss: 0.5374, acc: 0.7395\n",
      "loss: 0.5381, acc: 0.7388\n",
      "loss: 0.5373, acc: 0.7389\n",
      "loss: 0.5368, acc: 0.7393\n",
      "loss: 0.5374, acc: 0.7390\n",
      "loss: 0.5375, acc: 0.7387\n",
      "loss: 0.5369, acc: 0.7388\n",
      "loss: 0.5368, acc: 0.7392\n",
      "loss: 0.5372, acc: 0.7386\n",
      "loss: 0.5378, acc: 0.7373\n",
      "loss: 0.5381, acc: 0.7370\n",
      "loss: 0.5382, acc: 0.7368\n",
      "loss: 0.5392, acc: 0.7355\n",
      "loss: 0.5391, acc: 0.7356\n",
      "loss: 0.5394, acc: 0.7357\n",
      "loss: 0.5390, acc: 0.7361\n",
      "loss: 0.5400, acc: 0.7358\n",
      "loss: 0.5390, acc: 0.7369\n",
      "loss: 0.5385, acc: 0.7372\n",
      "loss: 0.5384, acc: 0.7373\n",
      "loss: 0.5383, acc: 0.7374\n",
      "loss: 0.5371, acc: 0.7381\n",
      "loss: 0.5368, acc: 0.7381\n",
      "> max_val_f1: 0.7133, max_val_acc: 0.7602\n",
      "> val_acc: 0.7436, val_f1: 0.6850, val_precision: 0.6700, val_recall: 0.7007\n",
      "loss: 0.5369, acc: 0.7385\n",
      "loss: 0.5368, acc: 0.7386\n",
      "loss: 0.5359, acc: 0.7392\n",
      "loss: 0.5359, acc: 0.7393\n",
      "loss: 0.5371, acc: 0.7387\n",
      "loss: 0.5357, acc: 0.7394\n",
      "loss: 0.5348, acc: 0.7397\n",
      "loss: 0.5344, acc: 0.7395\n",
      "loss: 0.5345, acc: 0.7392\n",
      "loss: 0.5333, acc: 0.7399\n",
      "loss: 0.5320, acc: 0.7408\n",
      "loss: 0.5308, acc: 0.7415\n",
      "loss: 0.5315, acc: 0.7406\n",
      "loss: 0.5319, acc: 0.7407\n",
      "loss: 0.5310, acc: 0.7413\n",
      "loss: 0.5308, acc: 0.7419\n",
      "loss: 0.5306, acc: 0.7422\n",
      "loss: 0.5324, acc: 0.7414\n",
      "loss: 0.5323, acc: 0.7409\n",
      "loss: 0.5315, acc: 0.7415\n",
      "loss: 0.5309, acc: 0.7415\n",
      "loss: 0.5307, acc: 0.7416\n",
      "loss: 0.5300, acc: 0.7422\n",
      "loss: 0.5290, acc: 0.7430\n",
      "loss: 0.5279, acc: 0.7439\n",
      "loss: 0.5270, acc: 0.7445\n",
      "loss: 0.5259, acc: 0.7453\n",
      "loss: 0.5253, acc: 0.7453\n",
      "loss: 0.5263, acc: 0.7454\n",
      "loss: 0.5263, acc: 0.7457\n",
      "loss: 0.5249, acc: 0.7465\n",
      "loss: 0.5251, acc: 0.7465\n",
      "loss: 0.5241, acc: 0.7470\n",
      "loss: 0.5239, acc: 0.7473\n",
      "loss: 0.5235, acc: 0.7479\n",
      "loss: 0.5234, acc: 0.7479\n",
      "loss: 0.5222, acc: 0.7487\n",
      "loss: 0.5224, acc: 0.7487\n",
      "loss: 0.5211, acc: 0.7492\n",
      "loss: 0.5211, acc: 0.7492\n",
      "> max_val_f1: 0.7133, max_val_acc: 0.7602\n",
      "> val_acc: 0.7950, val_f1: 0.7253, val_precision: 0.7771, val_recall: 0.6799\n",
      ">> saved: /hy-tmp/models/best_state/CM_ATTENTION4\n",
      "loss: 0.5210, acc: 0.7492\n",
      "loss: 0.5213, acc: 0.7492\n",
      "loss: 0.5211, acc: 0.7492\n",
      "loss: 0.5218, acc: 0.7490\n",
      "loss: 0.5217, acc: 0.7490\n",
      "loss: 0.5210, acc: 0.7495\n",
      "loss: 0.5210, acc: 0.7492\n",
      "loss: 0.5203, acc: 0.7497\n",
      "loss: 0.5203, acc: 0.7497\n",
      "loss: 0.5197, acc: 0.7502\n",
      "loss: 0.5193, acc: 0.7507\n",
      "loss: 0.5195, acc: 0.7505\n",
      "loss: 0.5196, acc: 0.7500\n",
      "loss: 0.5202, acc: 0.7500\n",
      "loss: 0.5197, acc: 0.7505\n",
      "loss: 0.5190, acc: 0.7505\n",
      "loss: 0.5190, acc: 0.7505\n",
      "loss: 0.5202, acc: 0.7495\n",
      "loss: 0.5194, acc: 0.7502\n",
      "loss: 0.5193, acc: 0.7505\n",
      "loss: 0.5189, acc: 0.7507\n",
      "loss: 0.5182, acc: 0.7512\n",
      "loss: 0.5179, acc: 0.7512\n",
      "loss: 0.5190, acc: 0.7509\n",
      "loss: 0.5186, acc: 0.7512\n",
      "loss: 0.5176, acc: 0.7519\n",
      "loss: 0.5167, acc: 0.7523\n",
      "loss: 0.5179, acc: 0.7519\n",
      "loss: 0.5167, acc: 0.7526\n",
      "loss: 0.5169, acc: 0.7523\n",
      "loss: 0.5171, acc: 0.7521\n",
      "loss: 0.5172, acc: 0.7521\n",
      "loss: 0.5183, acc: 0.7514\n",
      "loss: 0.5186, acc: 0.7511\n",
      "loss: 0.5182, acc: 0.7514\n",
      "loss: 0.5181, acc: 0.7518\n",
      "loss: 0.5182, acc: 0.7516\n",
      "loss: 0.5186, acc: 0.7513\n",
      "loss: 0.5185, acc: 0.7518\n",
      "loss: 0.5179, acc: 0.7527\n",
      "> max_val_f1: 0.7253, max_val_acc: 0.7950\n",
      "> val_acc: 0.7556, val_f1: 0.6326, val_precision: 0.7873, val_recall: 0.5287\n",
      "loss: 0.5175, acc: 0.7531\n",
      "loss: 0.5170, acc: 0.7533\n",
      "loss: 0.5168, acc: 0.7538\n",
      "loss: 0.5171, acc: 0.7537\n",
      "loss: 0.5169, acc: 0.7539\n",
      "loss: 0.5167, acc: 0.7537\n",
      "loss: 0.5162, acc: 0.7539\n",
      "loss: 0.5162, acc: 0.7537\n",
      "loss: 0.5155, acc: 0.7541\n",
      "loss: 0.5158, acc: 0.7539\n",
      "loss: 0.5154, acc: 0.7543\n",
      "loss: 0.5145, acc: 0.7549\n",
      "loss: 0.5141, acc: 0.7551\n",
      "loss: 0.5142, acc: 0.7553\n",
      "loss: 0.5142, acc: 0.7551\n",
      "loss: 0.5134, acc: 0.7555\n",
      "loss: 0.5137, acc: 0.7555\n",
      "loss: 0.5133, acc: 0.7559\n",
      "loss: 0.5131, acc: 0.7554\n",
      "loss: 0.5128, acc: 0.7558\n",
      "loss: 0.5131, acc: 0.7556\n",
      "loss: 0.5123, acc: 0.7562\n",
      "loss: 0.5119, acc: 0.7566\n",
      "loss: 0.5111, acc: 0.7572\n",
      "loss: 0.5109, acc: 0.7574\n",
      "loss: 0.5106, acc: 0.7576\n",
      "loss: 0.5104, acc: 0.7577\n",
      "loss: 0.5095, acc: 0.7583\n",
      "loss: 0.5082, acc: 0.7591\n",
      "loss: 0.5079, acc: 0.7595\n",
      "loss: 0.5076, acc: 0.7596\n",
      "loss: 0.5084, acc: 0.7590\n",
      "loss: 0.5082, acc: 0.7594\n",
      "loss: 0.5078, acc: 0.7596\n",
      "loss: 0.5079, acc: 0.7595\n",
      "loss: 0.5070, acc: 0.7601\n",
      "loss: 0.5069, acc: 0.7601\n",
      "loss: 0.5070, acc: 0.7596\n",
      "loss: 0.5064, acc: 0.7596\n",
      "loss: 0.5066, acc: 0.7596\n",
      "> max_val_f1: 0.7253, max_val_acc: 0.7950\n",
      "> val_acc: 0.7871, val_f1: 0.7201, val_precision: 0.7551, val_recall: 0.6882\n",
      "loss: 0.5066, acc: 0.7597\n",
      "loss: 0.5063, acc: 0.7597\n",
      "loss: 0.5058, acc: 0.7599\n",
      "loss: 0.5054, acc: 0.7602\n",
      "loss: 0.5049, acc: 0.7606\n",
      "loss: 0.5050, acc: 0.7602\n",
      "loss: 0.5042, acc: 0.7607\n",
      "loss: 0.5047, acc: 0.7607\n",
      "loss: 0.5042, acc: 0.7610\n",
      "loss: 0.5036, acc: 0.7616\n",
      "loss: 0.5037, acc: 0.7615\n",
      "loss: 0.5038, acc: 0.7613\n",
      "loss: 0.5036, acc: 0.7616\n",
      "loss: 0.5033, acc: 0.7616\n",
      "loss: 0.5024, acc: 0.7623\n",
      "loss: 0.5022, acc: 0.7626\n",
      "loss: 0.5019, acc: 0.7630\n",
      "loss: 0.5012, acc: 0.7633\n",
      "loss: 0.5013, acc: 0.7635\n",
      "loss: 0.5008, acc: 0.7634\n",
      "loss: 0.4997, acc: 0.7641\n",
      "loss: 0.4988, acc: 0.7648\n",
      "loss: 0.4987, acc: 0.7648\n",
      "loss: 0.4987, acc: 0.7651\n",
      "loss: 0.4982, acc: 0.7654\n",
      "loss: 0.4973, acc: 0.7657\n",
      "loss: 0.4974, acc: 0.7653\n",
      "loss: 0.4972, acc: 0.7653\n",
      "loss: 0.4970, acc: 0.7652\n",
      "loss: 0.4974, acc: 0.7652\n",
      "loss: 0.4972, acc: 0.7653\n",
      "loss: 0.4972, acc: 0.7653\n",
      "loss: 0.4972, acc: 0.7654\n",
      "loss: 0.4974, acc: 0.7652\n",
      "loss: 0.4967, acc: 0.7657\n",
      "loss: 0.4961, acc: 0.7660\n",
      "loss: 0.4957, acc: 0.7661\n",
      "loss: 0.4952, acc: 0.7664\n",
      "loss: 0.4945, acc: 0.7669\n",
      "loss: 0.4939, acc: 0.7674\n",
      "> max_val_f1: 0.7253, max_val_acc: 0.7950\n",
      "> val_acc: 0.8066, val_f1: 0.7682, val_precision: 0.7345, val_recall: 0.8050\n",
      ">> saved: /hy-tmp/models/best_state/CM_ATTENTION4\n",
      "loss: 0.4932, acc: 0.7678\n",
      "loss: 0.4929, acc: 0.7681\n",
      "loss: 0.4926, acc: 0.7686\n",
      "loss: 0.4923, acc: 0.7687\n",
      "loss: 0.4924, acc: 0.7685\n",
      "loss: 0.4924, acc: 0.7684\n",
      "loss: 0.4922, acc: 0.7687\n",
      "loss: 0.4927, acc: 0.7685\n",
      "loss: 0.4922, acc: 0.7688\n",
      "loss: 0.4915, acc: 0.7691\n",
      "loss: 0.4922, acc: 0.7687\n",
      "loss: 0.4917, acc: 0.7690\n",
      "loss: 0.4907, acc: 0.7696\n",
      "loss: 0.4911, acc: 0.7696\n",
      "loss: 0.4908, acc: 0.7697\n",
      "loss: 0.4907, acc: 0.7698\n",
      "loss: 0.4905, acc: 0.7699\n",
      "loss: 0.4899, acc: 0.7703\n",
      "loss: 0.4899, acc: 0.7706\n",
      "loss: 0.4892, acc: 0.7712\n",
      "loss: 0.4888, acc: 0.7717\n",
      "loss: 0.4886, acc: 0.7719\n",
      "loss: 0.4877, acc: 0.7725\n",
      "loss: 0.4879, acc: 0.7725\n",
      "loss: 0.4877, acc: 0.7727\n",
      "loss: 0.4875, acc: 0.7728\n",
      "loss: 0.4870, acc: 0.7731\n",
      "loss: 0.4862, acc: 0.7735\n",
      "loss: 0.4866, acc: 0.7735\n",
      "loss: 0.4865, acc: 0.7736\n",
      "loss: 0.4864, acc: 0.7737\n",
      "loss: 0.4856, acc: 0.7741\n",
      "loss: 0.4853, acc: 0.7742\n",
      "loss: 0.4847, acc: 0.7744\n",
      "loss: 0.4842, acc: 0.7747\n",
      "loss: 0.4840, acc: 0.7748\n",
      "loss: 0.4839, acc: 0.7749\n",
      "loss: 0.4834, acc: 0.7751\n",
      "loss: 0.4828, acc: 0.7755\n",
      "loss: 0.4827, acc: 0.7758\n",
      "> max_val_f1: 0.7682, max_val_acc: 0.8066\n",
      "> val_acc: 0.8108, val_f1: 0.7797, val_precision: 0.7264, val_recall: 0.8415\n",
      ">> saved: /hy-tmp/models/best_state/CM_ATTENTION4\n",
      "loss: 0.4821, acc: 0.7760\n",
      "loss: 0.4815, acc: 0.7764\n",
      "loss: 0.4815, acc: 0.7765\n",
      "loss: 0.4807, acc: 0.7769\n",
      "loss: 0.4804, acc: 0.7772\n",
      "loss: 0.4800, acc: 0.7772\n",
      "loss: 0.4797, acc: 0.7775\n",
      "loss: 0.4800, acc: 0.7776\n",
      "loss: 0.4798, acc: 0.7777\n",
      "loss: 0.4794, acc: 0.7777\n",
      "loss: 0.4790, acc: 0.7780\n",
      "loss: 0.4787, acc: 0.7781\n",
      "loss: 0.4781, acc: 0.7785\n",
      "loss: 0.4778, acc: 0.7785\n",
      "loss: 0.4773, acc: 0.7786\n",
      "loss: 0.4775, acc: 0.7784\n",
      "loss: 0.4774, acc: 0.7785\n",
      "loss: 0.4768, acc: 0.7789\n",
      "loss: 0.4768, acc: 0.7789\n",
      "loss: 0.4760, acc: 0.7793\n",
      "loss: 0.4762, acc: 0.7792\n",
      "loss: 0.4772, acc: 0.7789\n",
      "loss: 0.4767, acc: 0.7793\n",
      "loss: 0.4767, acc: 0.7793\n",
      "loss: 0.4768, acc: 0.7793\n",
      "loss: 0.4767, acc: 0.7793\n",
      "loss: 0.4765, acc: 0.7796\n",
      "loss: 0.4762, acc: 0.7798\n",
      "loss: 0.4756, acc: 0.7803\n",
      "loss: 0.4753, acc: 0.7807\n",
      "loss: 0.4748, acc: 0.7810\n",
      "loss: 0.4746, acc: 0.7811\n",
      "loss: 0.4739, acc: 0.7816\n",
      "loss: 0.4737, acc: 0.7815\n",
      "loss: 0.4744, acc: 0.7812\n",
      "loss: 0.4745, acc: 0.7810\n",
      "loss: 0.4741, acc: 0.7810\n",
      "loss: 0.4741, acc: 0.7810\n",
      "loss: 0.4736, acc: 0.7812\n",
      "loss: 0.4733, acc: 0.7814\n",
      "loss: 0.0331, acc: 0.9903\n",
      "loss: 0.0335, acc: 0.9900\n",
      "loss: 0.0337, acc: 0.9898\n",
      "loss: 0.0337, acc: 0.9899\n",
      "loss: 0.0336, acc: 0.9899\n",
      "loss: 0.0336, acc: 0.9899\n",
      "loss: 0.0336, acc: 0.9899\n",
      "loss: 0.0335, acc: 0.9900\n",
      "loss: 0.0336, acc: 0.9898\n",
      "loss: 0.0335, acc: 0.9899\n",
      "loss: 0.0334, acc: 0.9899\n",
      "loss: 0.0334, acc: 0.9899\n",
      "loss: 0.0333, acc: 0.9899\n",
      "loss: 0.0335, acc: 0.9898\n",
      "loss: 0.0334, acc: 0.9898\n",
      "loss: 0.0333, acc: 0.9898\n",
      "loss: 0.0332, acc: 0.9899\n",
      "loss: 0.0332, acc: 0.9899\n",
      "loss: 0.0339, acc: 0.9898\n",
      "loss: 0.0339, acc: 0.9898\n",
      "loss: 0.0338, acc: 0.9898\n",
      "loss: 0.0338, acc: 0.9898\n",
      "loss: 0.0337, acc: 0.9899\n",
      "loss: 0.0336, acc: 0.9899\n",
      "loss: 0.0335, acc: 0.9899\n",
      "loss: 0.0335, acc: 0.9899\n",
      "loss: 0.0335, acc: 0.9900\n",
      "loss: 0.0334, acc: 0.9900\n",
      "> max_val_f1: 0.8399, max_val_acc: 0.8726\n",
      "> val_acc: 0.8589, val_f1: 0.8240, val_precision: 0.8181, val_recall: 0.8300\n",
      "loss: 0.0319, acc: 0.9894\n",
      "loss: 0.0319, acc: 0.9894\n",
      "loss: 0.0323, acc: 0.9894\n",
      "loss: 0.0322, acc: 0.9894\n",
      "loss: 0.0324, acc: 0.9894\n",
      "loss: 0.0325, acc: 0.9893\n",
      "loss: 0.0325, acc: 0.9893\n",
      "loss: 0.0326, acc: 0.9893\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0332, acc: 0.9892\n",
      "loss: 0.0333, acc: 0.9891\n",
      "loss: 0.0333, acc: 0.9891\n",
      "loss: 0.0333, acc: 0.9891\n",
      "loss: 0.0333, acc: 0.9891\n",
      "loss: 0.0335, acc: 0.9890\n",
      "loss: 0.0335, acc: 0.9890\n",
      "loss: 0.0336, acc: 0.9890\n",
      "loss: 0.0336, acc: 0.9890\n",
      "loss: 0.0336, acc: 0.9890\n",
      "loss: 0.0336, acc: 0.9890\n",
      "loss: 0.0335, acc: 0.9890\n",
      "loss: 0.0335, acc: 0.9890\n",
      "loss: 0.0335, acc: 0.9891\n",
      "loss: 0.0338, acc: 0.9890\n",
      "loss: 0.0343, acc: 0.9888\n",
      "loss: 0.0344, acc: 0.9887\n",
      "loss: 0.0344, acc: 0.9888\n",
      "loss: 0.0344, acc: 0.9888\n",
      "loss: 0.0344, acc: 0.9887\n",
      "loss: 0.0345, acc: 0.9887\n",
      "loss: 0.0345, acc: 0.9887\n",
      "loss: 0.0347, acc: 0.9886\n",
      "loss: 0.0347, acc: 0.9886\n",
      "loss: 0.0347, acc: 0.9887\n",
      "loss: 0.0347, acc: 0.9886\n",
      "> max_val_f1: 0.8399, max_val_acc: 0.8726\n",
      "> val_acc: 0.8631, val_f1: 0.8338, val_precision: 0.8062, val_recall: 0.8634\n",
      "loss: 0.0347, acc: 0.9886\n",
      "loss: 0.0347, acc: 0.9886\n",
      "loss: 0.0349, acc: 0.9886\n",
      "loss: 0.0348, acc: 0.9886\n",
      "loss: 0.0348, acc: 0.9886\n",
      "loss: 0.0349, acc: 0.9886\n",
      "loss: 0.0349, acc: 0.9886\n",
      "loss: 0.0349, acc: 0.9886\n",
      "loss: 0.0349, acc: 0.9886\n",
      "loss: 0.0349, acc: 0.9886\n",
      "loss: 0.0350, acc: 0.9885\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9885\n",
      "loss: 0.0350, acc: 0.9885\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9885\n",
      "loss: 0.0351, acc: 0.9885\n",
      "loss: 0.0351, acc: 0.9885\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0351, acc: 0.9884\n",
      "loss: 0.0350, acc: 0.9884\n",
      "loss: 0.0350, acc: 0.9885\n",
      "loss: 0.0350, acc: 0.9885\n",
      "loss: 0.0350, acc: 0.9885\n",
      "loss: 0.0349, acc: 0.9885\n",
      "loss: 0.0349, acc: 0.9884\n",
      "loss: 0.0349, acc: 0.9884\n",
      "loss: 0.0349, acc: 0.9885\n",
      "loss: 0.0349, acc: 0.9885\n",
      "loss: 0.0348, acc: 0.9885\n",
      "loss: 0.0350, acc: 0.9884\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "> max_val_f1: 0.8399, max_val_acc: 0.8726\n",
      "> val_acc: 0.8539, val_f1: 0.8293, val_precision: 0.7752, val_recall: 0.8916\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0352, acc: 0.9883\n",
      "loss: 0.0352, acc: 0.9883\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9884\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0353, acc: 0.9883\n",
      "loss: 0.0353, acc: 0.9884\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0352, acc: 0.9884\n",
      "loss: 0.0354, acc: 0.9883\n",
      "loss: 0.0355, acc: 0.9883\n",
      "loss: 0.0355, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9882\n",
      "loss: 0.0358, acc: 0.9882\n",
      "loss: 0.0358, acc: 0.9882\n",
      "loss: 0.0358, acc: 0.9882\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9882\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9882\n",
      "loss: 0.0358, acc: 0.9882\n",
      "loss: 0.0357, acc: 0.9882\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "> max_val_f1: 0.8399, max_val_acc: 0.8726\n",
      "> val_acc: 0.8668, val_f1: 0.8283, val_precision: 0.8505, val_recall: 0.8071\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9883\n",
      "loss: 0.0360, acc: 0.9883\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0360, acc: 0.9881\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0360, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0359, acc: 0.9882\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0358, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0357, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0356, acc: 0.9883\n",
      "loss: 0.0355, acc: 0.9883\n",
      "loss: 0.0355, acc: 0.9883\n",
      "> max_val_f1: 0.8399, max_val_acc: 0.8726\n",
      "> val_acc: 0.8515, val_f1: 0.7993, val_precision: 0.8642, val_recall: 0.7435\n",
      "loss: 0.0355, acc: 0.9883\n",
      "loss: 0.0354, acc: 0.9884\n",
      "loss: 0.0364, acc: 0.9882\n",
      "loss: 0.0364, acc: 0.9882\n",
      "loss: 0.0364, acc: 0.9882\n",
      "loss: 0.0363, acc: 0.9882\n",
      "loss: 0.0363, acc: 0.9882\n",
      "loss: 0.0365, acc: 0.9881\n",
      "loss: 0.0365, acc: 0.9881\n",
      "loss: 0.0371, acc: 0.9880\n",
      "loss: 0.0371, acc: 0.9880\n",
      "loss: 0.0371, acc: 0.9880\n",
      "loss: 0.0370, acc: 0.9881\n",
      "loss: 0.0370, acc: 0.9881\n",
      "loss: 0.0370, acc: 0.9881\n",
      "loss: 0.0369, acc: 0.9881\n",
      "loss: 0.0370, acc: 0.9880\n",
      "loss: 0.0370, acc: 0.9881\n",
      "loss: 0.0372, acc: 0.9880\n",
      "loss: 0.0371, acc: 0.9880\n",
      "loss: 0.0371, acc: 0.9880\n",
      "loss: 0.0371, acc: 0.9880\n",
      "loss: 0.0370, acc: 0.9880\n",
      "loss: 0.0372, acc: 0.9880\n",
      "loss: 0.0373, acc: 0.9879\n",
      "loss: 0.0373, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      "loss: 0.0372, acc: 0.9879\n",
      ">> early stop.\n",
      "0.8584474885844748 0.8234075608493008 0.8179012345679012 0.8289885297184567\n",
      "(0.8726141078838174, 0.8398539384454878, 0.8402922755741128, 0.8394160583941606)\n",
      "(0.8584474885844748, 0.8234075608493008, 0.8179012345679012, 0.8289885297184567)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    model = CM_ATTENTION4(pretrained_bert_name, pretrained_vit_name).to(device)\n",
    "    \n",
    "    train(model, train_loader, valid_loader, test_loader)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_checkpoint))\n",
    "    model = model.to(device)\n",
    "    print(eval_(model, valid_loader, save_path=f'/root/results/{model_name}_val_predicts.txt'))\n",
    "    print(eval_(model, test_loader, save_path=result_file))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e89824-55cb-4e2a-9e68-ddcf8a14a843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
